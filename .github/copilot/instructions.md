# Guia de Mensagens de Commit para Copilot - Projeto Scraper

Este documento orienta humanos e IA (como o GitHub Copilot) a gerar mensagens de commit eficientes, consistentes e automatiz√°veis neste reposit√≥rio de web scraping.

---

## üéØ Formato Padr√£o Recomendado

```
tipo(escopo): descri√ß√£o curta

Descri√ß√£o longa detalhada (opcional).

Fixes #n / Closes #n (opcional)
BREAKING CHANGE: <descri√ß√£o> (se aplic√°vel)
```

- **tipo:** Veja lista e exemplos abaixo.  
- **escopo:** (opcional) Parte espec√≠fica afetada (spider, pipeline, middleware, item, etc).
- **descri√ß√£o curta:** Resuma a altera√ß√£o em at√© 50 caracteres, voz ativa e verbo no presente.
- **descri√ß√£o longa:** Contexto, motiva√ß√£o, impactos, detalhes t√©cnicos.
- **refer√™ncia a issues:** "Fixes #123" ou "Closes #123" fecha issues automaticamente.
- **breaking change:** Use "BREAKING CHANGE:" para mudan√ßas incompat√≠veis.

---

## üè∑Ô∏è Tipos de Commit e Exemplos

> Copilot: sempre prefira um tipo da lista abaixo; use exemplos como modelo!

- **feat:** Nova funcionalidade  
  `feat(spider): adiciona spider para coleta de departamentos`

- **feature:** Nova funcionalidade (alternativo a feat)  
  `feature(pipeline): implementa pipeline de valida√ß√£o de dados`

- **fix:** Corre√ß√£o de bug  
  `fix(docentes): corrige seletor CSS para dados de docentes`

- **docs:** Mudan√ßas na documenta√ß√£o  
  `docs(readme): atualiza guia de instala√ß√£o do Scrapy`

- **style:** Mudan√ßa de formata√ß√£o, lint, indenta√ß√£o (sem alterar l√≥gica)  
  `style(spider): ajusta formata√ß√£o PEP8`

- **refactor:** Refatora√ß√£o sem alterar funcionalidade  
  `refactor(items): simplifica estrutura de dados`

- **test:** Adiciona ou ajusta testes  
  `test(spider): adiciona testes unit√°rios para docentes spider`

- **ci:** Integra√ß√£o cont√≠nua (workflows, pipelines, etc)  
  `ci(github-actions): adiciona verifica√ß√£o de qualidade de c√≥digo`

- **chore:** Tarefas de build, configs, depend√™ncias  
  `chore(deps): atualiza requirements.txt`

- **perf:** Melhoria de performance  
  `perf(spider): otimiza rate limiting e concurrent requests`

- **cleanup:** Remove c√≥digo morto, logs, vari√°veis n√£o usadas  
  `cleanup: remove prints de debug obsoletos`

- **folders:** Cria√ß√£o, remo√ß√£o ou renomea√ß√£o de diret√≥rios  
  `folders: reorganiza estrutura de dados coletados`

- **files:** Cria√ß√£o, remo√ß√£o ou renomea√ß√£o de arquivos  
  `files: move mock data para pasta dedicada`

- **spider:** Mudan√ßas espec√≠ficas em spiders  
  `spider(docentes): adiciona extra√ß√£o de telefone e email`

- **pipeline:** Mudan√ßas em pipelines de processamento  
  `pipeline(validation): implementa valida√ß√£o de dados obrigat√≥rios`

- **middleware:** Mudan√ßas em middlewares  
  `middleware(headers): adiciona rota√ß√£o de user agents`

- **data:** Mudan√ßas em estruturas de dados ou arquivos de dados  
  `data(json): atualiza schema de dados de docentes`

- **config:** Mudan√ßas em configura√ß√µes do Scrapy  
  `config(settings): ajusta delay entre requests`

- **mock:** Adi√ß√£o ou atualiza√ß√£o de dados de teste/mock  
  `mock(sigaa): adiciona p√°gina HTML de exemplo`

- **scraping:** Melhorias gerais no processo de scraping  
  `scraping(sigaa): implementa retry para falhas de conex√£o`

- **outros tipos comuns:**  
  - **merge:** Mesclagem de branches  
  - **hotfix:** Corre√ß√£o cr√≠tica  
  - **build:** Mudan√ßas em build ou depend√™ncias externas  
  - **wip:** Trabalho em andamento  
  - **deps:** Atualiza√ß√£o de depend√™ncias  
  - **infra:** Infraestrutura  
  - **init:** Commit inicial  
  - **typo:** Corre√ß√£o de erros de digita√ß√£o  
  - **comment:** Coment√°rios no c√≥digo  
  - **optimize:** Otimiza√ß√£o  
  - **deprecate:** Deprecia√ß√£o  
  - **legal:** Licenciamento ou quest√µes legais  

---

## üõ†Ô∏è Boas Pr√°ticas para o Copilot

- Sempre respeite o formato: `<tipo>(<escopo>): <descri√ß√£o curta>`.
- Prefira mensagens espec√≠ficas e objetivas.  
  - **Ruim:** `fix: v√°rias corre√ß√µes`  
  - **Bom:** `fix(docentes): corrige extra√ß√£o de departamento quando campo est√° vazio`
- Use o escopo sempre que poss√≠vel: spider, pipeline, middleware, item, data, config.
- Nunca use pronomes pessoais ("eu", "n√≥s"). Sempre foque na a√ß√£o.
- Evite termos gen√©ricos como "melhoria", "atualiza√ß√£o", "altera√ß√£o".
- Se exceder 50 caracteres na descri√ß√£o curta, resuma e transfira detalhes para a descri√ß√£o longa.
- A descri√ß√£o longa deve explicar o porqu√™, o que e o como da altera√ß√£o.
- Para corre√ß√µes de issues, use "Fixes #num" ou "Closes #num" para fechar automaticamente issues relacionadas.
- Para breaking changes, detalhe claramente o impacto ap√≥s "BREAKING CHANGE:".
- Liste no corpo do commit as principais mudan√ßas (bullet points) se apropriado.
- N√£o use "commit", "mensagem de commit" ou "altera√ß√£o" na descri√ß√£o curta.

---

## ‚ö°Ô∏è Outras Orienta√ß√µes para o Copilot

Al√©m de gerar commits e descri√ß√µes de commits, siga estas diretrizes para automatizar e padronizar outras tarefas do reposit√≥rio:

### Pull Requests

- Escreva t√≠tulos objetivos para PRs no mesmo padr√£o dos commits (`tipo(escopo): descri√ß√£o curta`).
- Gere descri√ß√µes claras, explicando o que foi feito, por que e os impactos.
- Liste mudan√ßas principais em t√≥picos.
- Relacione issues com `Closes #num` quando aplic√°vel.

### Issues

- Siga templates claros para bugs, features e d√∫vidas.
- Gere t√≠tulos autom√°ticos objetivos e espec√≠ficos.
- Use estrutura de t√≥picos na descri√ß√£o para facilitar entendimento.

### Coment√°rios em C√≥digo

- Sugira coment√°rios construtivos, claros e objetivos em revis√µes.
- Explique bugs, d√∫vidas e sugest√µes de melhoria de forma direta.

### Releases

- Sugira descri√ß√µes de releases resumindo novidades, corre√ß√µes e breaking changes.
- Destaque os pontos principais em t√≥picos.

### Changelogs

- Gere changelogs autom√°ticos agrupando por tipo: features, fixes, refactors etc.
- Inclua refer√™ncias a issues/PRs relevantes.

### Nomes de Branches

#### Estrutura e padr√µes das `branchs`
- `main`: branch de **produ√ß√£o**, apenas c√≥digo final.
- `develop`: branch de **desenvolvimento**, para integra√ß√£o de features.
- `feature/*`: branches de **funcionalidades** (ex: `feature/docentes-spider`, `feature/validation-pipeline`).
- `fix/*`: branches de **corre√ß√µes** (ex: `fix/css-selector`, `fix/rate-limiting`).
- `hotfix/*`: branches de **corre√ß√µes cr√≠ticas** (ex: `hotfix/production-error`).

**Padronize nomes de branches** conforme exemplos acima.

### Documenta√ß√£o

- Gere documenta√ß√£o padr√£o para spiders, pipelines e middlewares.
- Sempre inclua: prop√≥sito, par√¢metros, dados extra√≠dos e exemplo de uso.

---

## ‚ûï Refor√ßos e Boas Pr√°ticas Adicionais

### üîí Seguran√ßa

- Nunca sugira inclus√£o de credenciais, tokens ou senhas no c√≥digo ou nos commits.
- Prefira sugerir o uso de vari√°veis de ambiente para dados sens√≠veis.
- Documente pr√°ticas de rate limiting para evitar sobrecarga dos servidores.

### üï∑Ô∏è Scrapy Best Practices

- Sempre respeite o robots.txt dos sites.
- Implemente delays apropriados entre requests.
- Use user agents realistas e rotativos.
- Trate erros de conex√£o e timeouts adequadamente.

### üìÅ Organiza√ß√£o de Projeto

- Siga a estrutura de diret√≥rios do Scrapy.
- Organize dados coletados em estruturas l√≥gicas (`data/spider_name/`).
- Mantenha arquivos mock separados dos dados reais.

### üß™ Testes

- Nomeie arquivos de teste conforme padr√£o Python (`test_*.py`).
- Sempre sugira criar ou atualizar testes ao adicionar ou modificar spiders.
- Use dados mock para testes consistentes.

### üìã Conven√ß√µes de C√≥digo

- Siga PEP8 para formata√ß√£o Python.
- Use docstrings para documentar classes e m√©todos.
- Prefira c√≥digo limpo, modular e reutiliz√°vel.

### üö® Mensagens de Erro e Logs

- Sugira logs informativos para debugging.
- Use n√≠veis de log apropriados (DEBUG, INFO, WARNING, ERROR).
- Inclua contexto suficiente nos logs para debugging.

### üìö Exemplos e Snippets

- Inspire-se nos exemplos deste guia e nos arquivos do projeto.
- Sugira snippets comuns ao contexto de web scraping.

### üè∑Ô∏è Labels & Automatiza√ß√µes

- Sugira labels padr√£o para issues/PRs (ex: `spider`, `pipeline`, `bug`, `enhancement`).
- Recomende automa√ß√µes via GitHub Actions para testes e qualidade de c√≥digo.

### üéØ Fluxo de Trabalho

- Oriente o fluxo para abertura de PRs, revis√µes e merges conforme pol√≠ticas do projeto.
- Sempre lembre de atualizar a documenta√ß√£o ap√≥s mudan√ßas relevantes.

### üìà M√©tricas e Monitoramento

- Sugira integra√ß√£o com ferramentas de monitoramento de scrapers.
- Recomende inclus√£o de m√©tricas de coleta (items/minute, success rate, etc.).

---

## üéì Contexto Espec√≠fico do Projeto Scraper

Este √© um projeto de **web scraping educacional/acad√™mico** focado na **coleta de dados do SIGAA (Sistema de Gest√£o Acad√™mica)** usando **Scrapy**. Considere estes aspectos espec√≠ficos:

### Tecnologias Principais
- **Scrapy:** Framework principal para web scraping
- **Python:** Linguagem de programa√ß√£o
- **JSON:** Formato de dados coletados
- **uv:** Gerenciador de depend√™ncias Python

### Estrutura de Diret√≥rios
- `sigaa/`: Projeto Scrapy principal
  - `spiders/`: Spiders para coleta de dados
  - `data/`: Dados coletados organizados
  - `mock/`: Dados de teste e p√°ginas de exemplo
- `aprender/`: M√≥dulos de aprendizado Python

### Escopos Espec√≠ficos Recomendados
- **docentes**: Para spider de docentes
- **departamentos**: Para dados de departamentos
- **sigaa**: Para configura√ß√µes gerais do SIGAA
- **spider**: Para mudan√ßas gerais em spiders
- **pipeline**: Para pipelines de processamento
- **middleware**: Para middlewares
- **data**: Para estruturas de dados
- **mock**: Para dados de teste
- **config**: Para configura√ß√µes

### Exemplos de Commits Contextualizados
```
feat(docentes): adiciona extra√ß√£o de dados de contato
fix(spider): corrige seletor CSS para departamentos
docs(readme): atualiza instru√ß√µes de instala√ß√£o
perf(pipeline): otimiza processamento de dados JSON
chore(deps): atualiza Scrapy para vers√£o 2.11
```

### Exemplo Completo de Commit com Descri√ß√£o Longa
```
feat(docentes): implementa spider completo para coleta de dados

Adiciona spider robusto para extrair informa√ß√µes de docentes do SIGAA:

- Spider: Implementa navega√ß√£o paginada e extra√ß√£o de dados estruturados
- Items: Define estrutura de dados com campos obrigat√≥rios e opcionais  
- Pipeline: Adiciona valida√ß√£o e limpeza de dados coletados
- Settings: Configura rate limiting e headers apropriados

- Mock: Adiciona p√°gina HTML de exemplo para testes
- Data: Organiza sa√≠da em formato JSON estruturado
- Tests: Implementa testes unit√°rios para valida√ß√£o

O spider extrai: nome, departamento, email, telefone, titula√ß√£o e √°rea de pesquisa.

Implementa retry autom√°tico para falhas de rede e respeita robots.txt.
```

---

## üéØ Conven√ß√µes Espec√≠ficas do Projeto

#### Estrutura de Arquivos
- Use **snake_case** para nomes de arquivos Python
- Mantenha dados organizados em `data/spider_name/`
- Use `mock/` para dados de teste e p√°ginas HTML de exemplo

#### Nomenclatura
- **Classes Spider:** Use sufixo `Spider` (ex: `DocentesSpider`)
- **Items:** Use sufixo `Item` (ex: `DocenteItem`)
- **Pipelines:** Use sufixo `Pipeline` (ex: `ValidationPipeline`)
- **Arquivos de dados:** Use nomes descritivos (ex: `docentes_2025.json`)

#### Scrapy/Spiders
- Sempre teste spiders com dados mock antes de executar
- Mantenha `settings.py` consistente e documentado
- Use campos obrigat√≥rios em Items para valida√ß√£o
- Documente seletores CSS/XPath complexos

#### Python/Processamento
- Use type hints quando poss√≠vel
- Adicione docstrings para classes e m√©todos principais
- Valide dados de entrada em pipelines
- Mantenha separa√ß√£o clara entre extra√ß√£o e processamento

---

## ü§ñ Dicas Extras para IAs

- Use exemplos reais do projeto (spiders, items, seletores) ao gerar escopo.
- Quando criar commits para m√∫ltiplos componentes Scrapy, prefira escopo mais amplo como "sigaa" ou "core".
- Sempre que poss√≠vel, relacione o commit a um issue ou PR.
- Para grandes refatora√ß√µes, use a se√ß√£o longa para explicar impactos na coleta de dados.
- Para automa√ß√µes, inclua o tipo "ci", "config" ou "chore" conforme o contexto.

---

## ü§ñ Marca√ß√£o de C√≥digo Gerado por IA

### Padr√£o Obrigat√≥rio para Identifica√ß√£o de IA

**TODOS os arquivos, trechos de c√≥digo, spiders ou pipelines gerados por IA devem ser marcados conforme os padr√µes abaixo:**

#### Para Arquivos Python (Spiders, Pipelines, Items)
```python
"""
Generated by AI (GitHub Copilot)
Date: 2025-07-04
Description: Spider para coleta de dados de docentes do SIGAA
"""

# Ou para trechos espec√≠ficos:
# AI-GENERATED START - Extra√ß√£o de dados de contato
def parse_contact_info(self, response):
    # c√≥digo gerado por IA
    pass
# AI-GENERATED END
```

#### Para Arquivos de Configura√ß√£o (settings.py, scrapy.cfg)
```python
# Generated by AI (GitHub Copilot)
# Date: 2025-07-04
# Description: Configura√ß√µes otimizadas para scraping do SIGAA

# Ou para se√ß√µes espec√≠ficas:
# AI-GENERATED START - Rate limiting settings
DOWNLOAD_DELAY = 3
RANDOMIZE_DOWNLOAD_DELAY = 0.5
# AI-GENERATED END
```

#### Para Arquivos JSON de Dados
```json
{
  "_metadata": {
    "generated_by": "AI (GitHub Copilot)",
    "date": "2025-07-04",
    "description": "Estrutura de dados para docentes"
  },
  "data": []
}
```

#### Para Documenta√ß√£o (.md, .txt)
```markdown
<!-- 
AI-GENERATED CONTENT
Generated by: GitHub Copilot
Date: 2025-07-04
Description: Documenta√ß√£o de uso do spider de docentes
-->

Ou para se√ß√µes espec√≠ficas:

<!-- AI-GENERATED START - Instru√ß√µes de instala√ß√£o -->
Conte√∫do gerado por IA...
<!-- AI-GENERATED END -->
```

### Regras de Marca√ß√£o

1. **Spiders Completamente Gerados por IA:**
   - Incluir docstring no topo da classe
   - Especificar data de gera√ß√£o
   - Incluir breve descri√ß√£o da funcionalidade de scraping

2. **M√©todos/Fun√ß√µes Parciais Gerados por IA:**
   - Usar coment√°rios `AI-GENERATED START/END`
   - Incluir descri√ß√£o do que foi gerado
   - Manter o c√≥digo humano sem marca√ß√£o

3. **Informa√ß√µes Obrigat√≥rias:**
   - Identifica√ß√£o: "Generated by AI (GitHub Copilot)"
   - Data de gera√ß√£o no formato YYYY-MM-DD
   - Descri√ß√£o breve da funcionalidade

4. **Casos Especiais:**
   - **Seletores CSS/XPath:** Sempre marcar se gerados por IA
   - **Pipelines de processamento:** Indicar claramente origem
   - **Configura√ß√µes cr√≠ticas:** Documentar origem das configura√ß√µes

### Benef√≠cios da Marca√ß√£o

- **Transpar√™ncia:** Clara identifica√ß√£o de c√≥digo gerado por IA
- **Manutenibilidade:** Facilita debugging de spiders e pipelines
- **Auditoria:** Permite rastreamento de l√≥gica de scraping
- **Compliance:** Atende requisitos de transpar√™ncia
- **Colabora√ß√£o:** Ajuda outros desenvolvedores a entender a origem do c√≥digo

### Exemplos Pr√°ticos do Projeto

```python
"""
Generated by AI (GitHub Copilot)
Date: 2025-07-04
Description: Spider para coleta de dados de docentes do SIGAA/UnB
"""

class DocentesSpider(scrapy.Spider):
    name = 'docentes'
    
    def parse(self, response):
        # AI-GENERATED START - Extra√ß√£o de lista de docentes
        docentes = response.css('.docente-item')
        for docente in docentes:
            # ... l√≥gica gerada por IA
        # AI-GENERATED END
        
        # Pagina√ß√£o implementada manualmente
        next_page = response.css('.next-page::attr(href)').get()
        if next_page:
            yield response.follow(next_page, self.parse)
```

```python
# Generated by AI (GitHub Copilot)
# Date: 2025-07-04
# Description: Configura√ß√µes otimizadas para scraping acad√™mico

# AI-GENERATED START - Settings de performance
CONCURRENT_REQUESTS = 16
DOWNLOAD_DELAY = 2
RANDOMIZE_DOWNLOAD_DELAY = 0.5
# AI-GENERATED END

# Configura√ß√£o manual de headers
DEFAULT_REQUEST_HEADERS = {
    'User-Agent': 'Academic Research Bot (+https://unb.br)'
}
```

---

## üîß Troubleshooting e Debugging

#### Problemas Comuns
- **Spider n√£o encontra elementos:** Verificar seletores CSS/XPath
- **Rate limiting/bloqueio:** Ajustar delays e headers
- **Dados inconsistentes:** Implementar valida√ß√£o em pipelines  
- **Timeouts frequentes:** Configurar retry middleware

#### Debugging Scrapy
- Use `scrapy shell` para testar seletores
- Ative logs DEBUG para an√°lise detalhada
- Teste com dados mock antes de executar em produ√ß√£o
- Use `response.css()` e `response.xpath()` no shell

#### Logs e Monitoramento
- Configure n√≠veis de log apropriados (INFO para produ√ß√£o)
- Monitore m√©tricas de coleta (items/min, errors, retries)
- Mantenha logs de execu√ß√£o para an√°lise posterior
- Documente erros recorrentes e suas solu√ß√µes

#### Performance
- Monitore uso de mem√≥ria em spiders grandes
- Otimize seletores CSS para melhor performance
- Use concurrent requests com modera√ß√£o
- Implemente cache quando apropriado
