# cd sigaa
# uv run scrapy runspider .\sigaa\spiders\docentes.py -o data\docentes\docentes.jsonl
import re
import json
import scrapy
import os
from pathlib import Path
import time


class DocentesSpider(scrapy.Spider):
    name = "docentes"
    allowed_domains = ["sigaa.unb.br"]
    start_urls = [
        "https://sigaa.unb.br/sigaa/public/docente/busca_docentes.jsf"]

    # Configura√ß√µes para processar um por vez
    custom_settings = {
        'DOWNLOAD_DELAY': 1,
        'CONCURRENT_REQUESTS': 1,  # Apenas 1 requisi√ß√£o por vez
        'CONCURRENT_REQUESTS_PER_DOMAIN': 1,
        'AUTOTHROTTLE_ENABLED': True,
        'AUTOTHROTTLE_START_DELAY': 1,
        'AUTOTHROTTLE_MAX_DELAY': 3,
        'AUTOTHROTTLE_TARGET_CONCURRENCY': 1.0,
    }

    def __init__(self):
        # Configurar pasta tempor√°ria
        self.temp_dir = Path("temp/current_dept")
        self.temp_dir.mkdir(parents=True, exist_ok=True)

        # Limpar arquivos antigos
        self._limpar_pasta_temp()

        # Lista de departamentos para processar sequencialmente
        self.departamentos_fila = []
        self.departamento_atual = 0

        self.logger.info(
            f"üöÄ Spider iniciado - Pasta tempor√°ria: {self.temp_dir.absolute()}")
        self.logger.info(f"üìù Modo: Processamento sequencial (1 por vez)")

    def parse(self, response):
        """Coleta lista de departamentos e inicia processamento sequencial"""
        departamentos = response.css("select#form\\:departamento option")

        # Montar fila de departamentos
        for opcao in departamentos:
            valor = opcao.attrib.get("value")
            texto = opcao.css("::text").get()

            if valor and valor not in ["", "0"]:
                self.departamentos_fila.append({
                    'id': valor,
                    'nome': texto.strip()
                })

        total_departamentos = len(self.departamentos_fila)
        self.logger.info(
            f"üìã Total de departamentos encontrados: {total_departamentos}")

        # Iniciar processamento do primeiro departamento
        if self.departamentos_fila:
            yield self.processar_proximo_departamento(response)

    def processar_proximo_departamento(self, response):
        """Processa o pr√≥ximo departamento da fila"""
        if self.departamento_atual >= len(self.departamentos_fila):
            self.logger.info("‚úÖ Todos os departamentos foram processados!")
            return

        dept = self.departamentos_fila[self.departamento_atual]
        self.logger.info(
            f"üì• [{self.departamento_atual + 1}/{len(self.departamentos_fila)}] Baixando: {dept['nome']}")

        return scrapy.FormRequest.from_response(
            response,
            formname='form',
            formdata={
                'form:departamento': dept['id'],
                'form:buscar': 'Buscar'
            },
            callback=self.salvar_e_processar_departamento,
            meta={
                'departamento_nome': dept['nome'],
                'departamento_id': dept['id'],
                'response_original': response
            }
        )

    def salvar_e_processar_departamento(self, response):
        """ETAPA 1: Salva HTML da tabela e chama processamento"""
        departamento_nome = response.meta['departamento_nome']
        departamento_id = response.meta['departamento_id']
        response_original = response.meta['response_original']

        # Verificar se h√° tabela de resultados
        tabela_resultados = response.css("table.listagem")
        if not tabela_resultados:
            self.logger.info(
                f"‚ùå Nenhum docente encontrado para {departamento_nome}")
            # Avan√ßar para pr√≥ximo departamento
            self.departamento_atual += 1
            yield self.processar_proximo_departamento(response_original)
            return

        # Extrair apenas o HTML da tabela
        tabela_html = tabela_resultados.get()

        # Salvar em arquivo tempor√°rio
        arquivo_temp = self.temp_dir / f"dept_{departamento_id}.html"
        with open(arquivo_temp, 'w', encoding='utf-8') as f:
            f.write(f"""<!DOCTYPE html>
<html><head><meta charset="UTF-8"></head><body>
<!-- Departamento: {departamento_nome} (ID: {departamento_id}) -->
{tabela_html}
</body></html>""")

        self.logger.info(f"üíæ HTML salvo: {arquivo_temp.name}")

        # ETAPA 2: Processar arquivo local
        yield from self.extrair_dados_locais(arquivo_temp, departamento_nome, departamento_id, response_original)

    def extrair_dados_locais(self, arquivo_html, departamento_nome, departamento_id, response_original):
        """ETAPA 2: Extrai dados do HTML local"""
        self.logger.info(f"‚öôÔ∏è Extraindo dados de {departamento_nome}...")

        # Ler HTML local
        with open(arquivo_html, 'r', encoding='utf-8') as f:
            html_content = f.read()

        # Criar response fict√≠cio para usar seletores CSS
        from scrapy.http import HtmlResponse
        local_response = HtmlResponse(
            url="file:///" + str(arquivo_html),
            body=html_content,
            encoding='utf-8'
        )

        # Extrair docentes usando mesma l√≥gica
        linhas_docentes = local_response.css("table.listagem tbody tr")
        total_docentes = len(linhas_docentes)
        self.logger.info(
            f"üìä Encontrados {total_docentes} docentes em {departamento_nome}")

        for linha in linhas_docentes:
            nome = linha.css("td:nth-child(2) span.nome::text").get()

            if nome:
                link_pagina = linha.css(
                    "td:nth-child(2) span.pagina a::attr(href)").get()

                siape = None
                if link_pagina and "siape=" in link_pagina:
                    siape = link_pagina.split("siape=")[1].split("&")[0]

                foto_url = linha.css("td.foto img::attr(src)").get()

                yield {
                    'codigo_departamento': departamento_id,
                    'departamento': departamento_nome,
                    'nome_docente': nome.strip(),
                    'siape': siape,
                    'link_pagina': link_pagina,
                    'processamento': 'sequencial_local'
                }

        # ETAPA 3: Limpar arquivo e avan√ßar
        yield from self.limpar_e_avancar(arquivo_html, departamento_nome, response_original)

    def limpar_e_avancar(self, arquivo_html, departamento_nome, response_original):
        """ETAPA 3: Remove arquivo tempor√°rio e avan√ßa para pr√≥ximo"""
        # Remover arquivo tempor√°rio
        try:
            arquivo_html.unlink()
            self.logger.info(f"üóëÔ∏è Arquivo removido: {arquivo_html.name}")
        except Exception as e:
            self.logger.warning(f"‚ö†Ô∏è Erro ao remover arquivo: {e}")

        # Pausa opcional entre departamentos
        time.sleep(0.5)

        # Avan√ßar para pr√≥ximo departamento
        self.departamento_atual += 1
        self.logger.info(f"‚û°Ô∏è Conclu√≠do: {departamento_nome}")

        # Processar pr√≥ximo ou finalizar
        if self.departamento_atual < len(self.departamentos_fila):
            yield self.processar_proximo_departamento(response_original)
        else:
            self.logger.info(
                f"üéâ CONCLU√çDO! Processados {len(self.departamentos_fila)} departamentos")

    def closed(self, reason):
        """M√©todo chamado quando spider √© finalizado"""
        # Limpar pasta tempor√°ria final
        try:
            if self.temp_dir.exists():
                # Remover arquivos restantes
                for arquivo in self.temp_dir.glob("*.html"):
                    arquivo.unlink()
                    self.logger.info(
                        f"üóëÔ∏è Limpeza final: removido {arquivo.name}")

                # Remover pasta se vazia
                if not any(self.temp_dir.iterdir()):
                    self.temp_dir.rmdir()
                    self.logger.info(
                        f"üìÅ Pasta tempor√°ria removida: {self.temp_dir}")
        except Exception as e:
            self.logger.warning(f"‚ö†Ô∏è Erro na limpeza final: {e}")

        # Estat√≠sticas finais
        total_processados = self.departamento_atual
        self.logger.info(f"üìà ESTAT√çSTICAS FINAIS:")
        self.logger.info(
            f"   üìã Departamentos processados: {total_processados}/{len(self.departamentos_fila)}")
        self.logger.info(f"   üèÅ Motivo de encerramento: {reason}")

    def _limpar_pasta_temp(self):
        """Limpa arquivos antigos da pasta tempor√°ria"""
        try:
            for arquivo in self.temp_dir.glob("*.html"):
                arquivo.unlink()
        except Exception as e:
            self.logger.warning(f"‚ö†Ô∏è Erro ao limpar pasta: {e}")


class DocentesConteudoSpider(scrapy.Spider):
    name = "docentes_conteudo"
    allowed_domains = ["sigaa.unb.br"]

    # Configura√ß√µes para processar um por vez
    custom_settings = {
        'DOWNLOAD_DELAY': 2,
        'CONCURRENT_REQUESTS': 1,
        'CONCURRENT_REQUESTS_PER_DOMAIN': 1,
        'AUTOTHROTTLE_ENABLED': True,
        'AUTOTHROTTLE_START_DELAY': 2,
        'AUTOTHROTTLE_MAX_DELAY': 5,
        'AUTOTHROTTLE_TARGET_CONCURRENCY': 1.0,
        'FEEDS': {
            'data/docentes/docentes_detalhes.jsonl': {
                'format': 'jsonlines',
                'encoding': 'utf8',
                'store_empty': False,
            },
        }
    }

    def __init__(self):
        # Pasta para salvar HTMLs das p√°ginas
        self.paginas_dir = Path("data/docentes/paginas_html")
        self.paginas_dir.mkdir(parents=True, exist_ok=True)

        self.total_docentes = 0
        self.processados = 0

        self.logger.info(
            f"üöÄ Spider iniciado - Salvando HTMLs em: {self.paginas_dir.absolute()}")

    def start_requests(self):
        """L√™ arquivo docentes.jsonl e gera requests para cada docente"""
        arquivo_docentes = Path("data/docentes/docentes.jsonl")

        if not arquivo_docentes.exists():
            self.logger.error(f"‚ùå Arquivo n√£o encontrado: {arquivo_docentes}")
            return

        docentes = []

        # Ler arquivo JSONL linha por linha
        with open(arquivo_docentes, 'r', encoding='utf-8') as f:
            for linha in f:
                linha = linha.strip()
                if linha:
                    try:
                        docente = json.loads(linha)
                        docentes.append(docente)
                    except json.JSONDecodeError as e:
                        self.logger.warning(f"‚ö†Ô∏è Erro ao parsear linha: {e}")

        self.total_docentes = len(docentes)
        self.logger.info(
            f"üìã Total de docentes encontrados: {self.total_docentes}")

        # Gerar requests para cada docente
        for i, docente in enumerate(docentes):
            link_pagina = docente.get("link_pagina")

            if link_pagina:
                # Construir URL completa
                if link_pagina.startswith("/"):
                    url = f"https://sigaa.unb.br{link_pagina}"
                else:
                    url = link_pagina

                yield scrapy.Request(
                    url=url,
                    callback=self.parse_docente,
                    meta={
                        "docente": docente,
                        "posicao": i + 1
                    },
                    dont_filter=True  # Permitir URLs duplicadas se houver
                )
            else:
                self.logger.warning(
                    f"‚ö†Ô∏è Docente sem link: {docente.get('nome_docente', 'N/A')}")

    def parse_docente(self, response):
        """Extrai informa√ß√µes da p√°gina do docente"""
        docente_original = response.meta["docente"]
        posicao = response.meta["posicao"]

        # Log de progresso
        self.logger.info(
            f"üì• [{posicao}/{self.total_docentes}] Processando: {docente_original.get('nome_docente', 'N/A')}")

        # Salvar HTML da p√°gina
        siape = docente_original.get("siape", "unknown")
        arquivo_html = self.paginas_dir / f"docente_{siape}.html"

        with open(arquivo_html, "w", encoding="utf-8") as f:
            f.write(response.text)

        # Extrair informa√ß√µes da p√°gina
        dados_extraidos = self.extrair_dados_pagina(response)

        # Combinar dados originais com dados extra√≠dos
        dados_completos = {
            **docente_original,  # Dados do JSONL original
            **dados_extraidos,  # Dados extra√≠dos da p√°gina
            "url_acessada": response.url,
            "arquivo_html_salvo": str(arquivo_html),
            "status_processamento": "sucesso"
        }

        self.processados += 1
        self.logger.info(
            f"‚úÖ [{self.processados}/{self.total_docentes}] Conclu√≠do: {dados_completos.get('nome_completo', 'N/A')}")

        yield dados_completos

    def extrair_dados_pagina(self, response):
        """Extrai informa√ß√µes espec√≠ficas da p√°gina do docente"""
        dados = {}

        try:
            # Nome completo (do t√≠tulo da p√°gina)
            nome_completo = response.css("#id-docente h3::text").get()
            if nome_completo:
                dados["nome_completo"] = nome_completo.strip().title()

            # Departamento completo
            departamento_completo = response.css(
                "#id-docente p.departamento::text").get()
            if departamento_completo:
                dados["departamento_completo"] = departamento_completo.strip()

            # Foto do docente
            foto_url = response.css(
                "#left .foto_professor img::attr(src)").get()
            if foto_url:
                if foto_url.startswith("/") or foto_url.startswith("https://"):
                    dados["foto_url"] = foto_url
                else:
                    dados["foto_url"] = f"https://sigaa.unb.br{foto_url}"

            # Situa√ß√£o do docente
            situacao = response.css("#left h3.situacao::text").get()
            if situacao:
                dados["situacao"] = situacao.strip()

            # === PERFIL PESSOAL ===
            perfil_section = response.css("#perfil-docente")
            if perfil_section:

                # Descri√ß√£o pessoal
                descricao = perfil_section.css(
                    "dl:contains('Descri√ß√£o pessoal') dd::text").get()
                if descricao:
                    dados["descricao_pessoal"] = descricao.strip()

                # Forma√ß√£o acad√™mica/profissional
                formacao = perfil_section.css(
                    "dl:contains('Forma√ß√£o acad√™mica') dd::text").get()
                if formacao:
                    dados["formacao_academica"] = formacao.strip()

                # √Åreas de interesse
                areas = perfil_section.css(
                    "dl:contains('√Åreas de Interesse') dd::text").get()
                if areas:
                    dados["areas_interesse"] = areas.strip()

                # Curr√≠culo Lattes
                lattes_link = perfil_section.css(
                    "dl:contains('Curr√≠culo Lattes') dd a::attr(href)").get()
                if lattes_link:
                    dados["curriculo_lattes"] = lattes_link.strip()

            # === FORMA√á√ÉO ACAD√äMICA ===
            formacao_section = response.css("#formacao-academica")
            if formacao_section:
                formacao_detalhes = {}

                # Extrair cada n√≠vel de forma√ß√£o
                for dl in formacao_section.css("dl"):
                    nivel = dl.css("dt span.ano::text").get()
                    detalhes = dl.css("dd::text").getall()

                    if nivel and detalhes:
                        nivel_clean = nivel.strip()
                        detalhes_clean = " | ".join(
                            [d.strip() for d in detalhes if d.strip()])
                        formacao_detalhes[nivel_clean] = detalhes_clean

                if formacao_detalhes:
                    dados["formacao_detalhada"] = formacao_detalhes

            # === CONTATOS ===
            contato_section = response.css("#contato")
            if contato_section:

                # Endere√ßo profissional
                endereco = contato_section.css(
                    "dl:contains('Endere√ßo profissional') dd::text").get()
                if endereco and "n√£o informado" not in endereco.lower():
                    dados["endereco_profissional"] = endereco.strip()

                # Sala
                sala = contato_section.css(
                    "dl:contains('Sala') dd::text").get()
                if sala and "n√£o informado" not in sala.lower():
                    dados["sala"] = sala.strip()

                # Telefone/Ramal
                telefone = contato_section.css(
                    "dl:contains('Telefone') dd::text").get()
                if telefone and "n√£o informado" not in telefone.lower():
                    dados["telefone_ramal"] = telefone.strip()

                # Email
                email = contato_section.css(
                    "dl:contains('Endere√ßo eletr√¥nico') dd::text").get()
                if email and "n√£o informado" not in email.lower():
                    dados["email"] = email.strip()

            # === DADOS DO CURR√çCULO LATTES (JavaScript) ===
            script_content = response.css(
                "script:contains('var curriculo')::text").get()
            if script_content:
                curriculo_data = self.extrair_curriculo_lattes(script_content)
                if curriculo_data:
                    dados["curriculo_lattes_dados"] = curriculo_data

        except Exception as e:
            self.logger.error(f"‚ùå Erro ao extrair dados: {e}")
            dados["erro_extracao"] = str(e)

        return dados

    def extrair_curriculo_lattes(self, script_content):
        """Extrai dados b√°sicos do curr√≠culo Lattes do JavaScript"""
        try:
            # Procurar padr√µes espec√≠ficos no script
            curriculo_info = {}

            # Data de atualiza√ß√£o
            match_data = re.search(
                r'"dataatualizacao":\s*"(\d+)"', script_content)
            if match_data:
                data_raw = match_data.group(1)
                # Converter formato DDMMAAAA para DD/MM/AAAA
                if len(data_raw) == 8:
                    curriculo_info["data_atualizacao"] = f"{data_raw[:2]}/{data_raw[2:4]}/{data_raw[4:]}"

            # Nome em cita√ß√µes bibliogr√°ficas
            match_citacao = re.search(
                r'"nomeemcitacoesbibliograficas":\s*"([^"]+)"', script_content)
            if match_citacao:
                curriculo_info["nome_citacoes"] = match_citacao.group(1)

            # Resumo CV
            match_resumo = re.search(
                r'"textoresumocvrh":\s*"([^"]+)"', script_content)
            if match_resumo:
                resumo = match_resumo.group(1)
                # Limpar HTML entities b√°sicas
                resumo = resumo.replace("&#201;", "√â").replace(
                    "&#195;", "√É").replace("&#231;", "√ß")
                curriculo_info["resumo_cv"] = resumo[:500] + \
                    "..." if len(resumo) > 500 else resumo

            return curriculo_info if curriculo_info else None

        except Exception as e:
            self.logger.warning(f"‚ö†Ô∏è Erro ao extrair curr√≠culo Lattes: {e}")
            return None

    def closed(self, reason):
        """M√©todo chamado quando spider √© finalizado"""
        self.logger.info(f"üìà ESTAT√çSTICAS FINAIS:")
        self.logger.info(f"   üìã Total de docentes: {self.total_docentes}")
        self.logger.info(f"   ‚úÖ Processados com sucesso: {self.processados}")
        self.logger.info(f"   üíæ HTMLs salvos em: {self.paginas_dir}")
        self.logger.info(f"   üèÅ Motivo de encerramento: {reason}")

        if self.processados < self.total_docentes:
            falhas = self.total_docentes - self.processados
            self.logger.warning(f"   ‚ö†Ô∏è Falhas/N√£o processados: {falhas}")

